#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
src/file_processors.py - ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å°‚ç”¨ã‚¯ãƒ©ã‚¹ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰
ZIPãƒ•ã‚¡ã‚¤ãƒ«å†…ã®Shapefileå‡¦ç†ã«å¯¾å¿œ
å¤§å­—ãƒ»ä¸ç›®ã®æ–‡å­—åˆ—è¡¨ç¤ºã‚’æ”¹å–„
"""

import streamlit as st
import pandas as pd
import geopandas as gpd
import zipfile
import io
import tempfile
import os
import re
from typing import Dict, List, Any, Optional

class FileProcessor:
    """ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.supported_extensions = ['.zip', '.csv', '.xlsx', '.xls', '.shp', '.kml', '.geojson']
        self.shapefile_extensions = ['.shp', '.dbf', '.shx', '.prj', '.cpg']

    def process_file(self, file_content: bytes, file_name: str, file_extension: str) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰"""
        try:
            st.write(f"ğŸ“ å‡¦ç†é–‹å§‹: {file_name} ({file_extension})")
            
            if file_extension == '.zip':
                return self._process_zip_file(file_content, file_name)
            elif file_extension in ['.csv', '.xlsx', '.xls']:
                return self._process_data_file(file_content, file_extension)
            elif file_extension == '.shp':
                return self._process_shapefile(file_content, file_name)
            elif file_extension in ['.kml', '.geojson']:
                return self._process_gis_file(file_content, file_extension)
            else:
                st.warning(f"âš ï¸ æœªå¯¾å¿œã®æ‹¡å¼µå­: {file_extension}")
                return False
                
        except Exception as e:
            st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

    def _normalize_area_name(self, name: str) -> str:
        """ã‚¨ãƒªã‚¢åã‚’æ­£è¦åŒ–ï¼ˆæ•°å­—ã‚³ãƒ¼ãƒ‰å¯¾å¿œï¼‰"""
        try:
            if not name or pd.isna(name):
                return ""
            
            name_str = str(name).strip()
            
            # ç©ºæ–‡å­—ã‚„"nan"ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if not name_str or name_str.lower() == 'nan':
                return ""
            
            # æ•°å­—ã®ã¿ã®å ´åˆã®å‡¦ç†
            if name_str.isdigit():
                # å°ã•ã„æ•°å­—ï¼ˆ1-20ï¼‰ã¯ä¸ç›®ã¨ã—ã¦å‡¦ç†
                if int(name_str) <= 20:
                    return f"{name_str}ä¸ç›®"
                else:
                    # å¤§ãã„æ•°å­—ã¯ã‚³ãƒ¼ãƒ‰å¤‰æ›ã‚’è©¦è¡Œ
                    return self._convert_area_code(name_str)
            
            # æ²–ç¸„çœŒã®å¤§å­—ã‚³ãƒ¼ãƒ‰å¤‰æ›
            converted = self._convert_area_code(name_str)
            if converted != name_str:
                return converted
            
            # æ—¢ã«é©åˆ‡ãªæ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
            return name_str
            
        except Exception as e:
            st.warning(f"âš ï¸ ã‚¨ãƒªã‚¢åæ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼ ({name}): {str(e)}")
            return str(name) if name else ""

    def _convert_area_code(self, code: str) -> str:
        """ã‚¨ãƒªã‚¢ã‚³ãƒ¼ãƒ‰ã‚’æ–‡å­—åˆ—ã«å¤‰æ›"""
        try:
            # æ²–ç¸„çœŒã®å¤§å­—ãƒ»åœ°åŒºã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
            okinawa_patterns = {
                # é‚£è¦‡å¸‚ã®å¤§å­—ä¾‹
                '01': 'é‚£è¦‡',
                '02': 'é¦–é‡Œ', 
                '03': 'çœŸå˜‰æ¯”',
                '04': 'æ³Š',
                '05': 'ä¹…èŒ‚åœ°',
                '06': 'ç‰§å¿—',
                '07': 'å®‰é‡Œ',
                '08': 'ä¸ŠåŸ',
                '09': 'å¤å³¶',
                '10': 'éŠ˜è‹…',
                # æµ¦æ·»å¸‚ã®å¤§å­—ä¾‹
                '11': 'å®®é‡Œ',
                '12': 'æ™®å¤©é–“',
                '13': 'å†…é–“',
                '14': 'çµŒå¡š',
                '15': 'æ¸¯å·',
                '16': 'ç‰§æ¸¯',
                # å®œé‡æ¹¾å¸‚ã®å¤§å­—ä¾‹
                '21': 'å¤§å±±',
                '22': 'å®œé‡æ¹¾',
                '23': 'æ–°åŸ',
                '24': 'æˆ‘å¦‚å¤',
                '25': 'å˜‰æ•°',
                '26': 'çœŸæ „åŸ'
            }
            
            # ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯
            padded_code = code.zfill(2)
            if padded_code in okinawa_patterns:
                return okinawa_patterns[padded_code]
            
            # å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
            if code in okinawa_patterns:
                return okinawa_patterns[code]
            
            # å¤‰æ›ã§ããªã„å ´åˆã¯å…ƒã®å€¤ã‚’è¿”ã™
            return code
            
        except Exception as e:
            return code

    def _process_zip_file(self, zip_content: bytes, zip_name: str) -> bool:
        """ZIPãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰"""
        try:
            st.write(f"ğŸ“¦ ZIPå‡¦ç†é–‹å§‹: {zip_name}")
            
            with tempfile.TemporaryDirectory() as temp_dir:
                # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£å‡
                with zipfile.ZipFile(io.BytesIO(zip_content)) as zip_file:
                    zip_file.extractall(temp_dir)
                    file_list = zip_file.namelist()
                    
                    st.write(f"ğŸ“¦ ZIPå†…ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ ({len(file_list)}å€‹):")
                    for file in file_list[:10]:
                        st.write(f"  ğŸ“„ {file}")
                    if len(file_list) > 10:
                        st.write(f"  ... ä»–{len(file_list)-10}å€‹")
                
                # å®Ÿéš›ã«è§£å‡ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
                extracted_files = []
                for root, dirs, files in os.walk(temp_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        rel_path = os.path.relpath(file_path, temp_dir)
                        extracted_files.append({
                            'name': file,
                            'path': file_path,
                            'relative_path': rel_path,
                            'extension': '.' + file.lower().split('.')[-1] if '.' in file else ''
                        })
                
                st.write(f"ğŸ—‚ï¸ è§£å‡ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« ({len(extracted_files)}å€‹):")
                for file_info in extracted_files[:10]:
                    st.write(f"  ğŸ“„ {file_info['name']} ({file_info['extension']})")
                if len(extracted_files) > 10:
                    st.write(f"  ... ä»–{len(extracted_files)-10}å€‹")
                
                # å‡¦ç†å¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é †æ¬¡ãƒã‚§ãƒƒã‚¯
                success = False
                
                # 1. Shapefileã‚’æœ€å„ªå…ˆã§å‡¦ç†
                success = self._try_process_shapefiles(extracted_files, temp_dir)
                if success:
                    return True
                
                # 2. CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
                success = self._try_process_csv_files(extracted_files, temp_dir)
                if success:
                    return True
                
                # 3. Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
                success = self._try_process_excel_files(extracted_files, temp_dir)
                if success:
                    return True
                
                # 4. ãã®ä»–ã®GISãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
                success = self._try_process_other_gis_files(extracted_files, temp_dir)
                if success:
                    return True
                
                # 5. å…¨ã¦å¤±æ•—ã—ãŸå ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ç¶™ç¶š
                st.warning("âš ï¸ å‡¦ç†å¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ãŒã€ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ç¶™ç¶šã—ã¾ã™")
                return self._create_dummy_area_data(zip_name)
                
        except zipfile.BadZipFile:
            st.error("âŒ ç„¡åŠ¹ãªZIPãƒ•ã‚¡ã‚¤ãƒ«ã§ã™")
            return False
        except Exception as e:
            st.error(f"âŒ ZIPå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

    def _try_process_shapefiles(self, extracted_files: List[Dict], temp_dir: str) -> bool:
        """Shapefileã®å‡¦ç†ã‚’è©¦è¡Œ"""
        try:
            # Shapefileã‚’æ¢ã™
            shp_files = [f for f in extracted_files if f['extension'] == '.shp']
            
            if not shp_files:
                st.info("â„¹ï¸ ShapefileãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            st.write(f"ğŸ—ºï¸ Shapefileç™ºè¦‹: {len(shp_files)}å€‹")
            for shp_file in shp_files:
                st.write(f"  ğŸ“ {shp_file['name']}")
            
            # æœ€åˆã®Shapefileã‚’å‡¦ç†
            shp_file = shp_files[0]
            shp_path = shp_file['path']
            
            st.write(f"ğŸ“¥ Shapefileèª­ã¿è¾¼ã¿: {shp_file['name']}")
            
            # GeoPandasã§èª­ã¿è¾¼ã¿
            gdf = gpd.read_file(shp_path)
            
            st.write(f"ğŸ“Š Shapefileæƒ…å ±:")
            st.write(f"  - ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(gdf):,}")
            st.write(f"  - åˆ—æ•°: {len(gdf.columns)}")
            st.write(f"  - åº§æ¨™ç³»: {gdf.crs}")
            
            # åˆ—åã‚’è¡¨ç¤º
            st.write(f"  - åˆ—å: {', '.join(gdf.columns[:10])}")
            if len(gdf.columns) > 10:
                st.write(f"    ... ä»–{len(gdf.columns)-10}å€‹")
            
            # å¤§å­—ãƒ»ä¸ç›®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            area_data = self._extract_area_data_from_gdf(gdf)
            if area_data:
                st.session_state.area_data = area_data
                st.success(f"âœ… å¤§å­—ãƒ»ä¸ç›®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º: {len(area_data)}å€‹")
                return True
            else:
                st.info("â„¹ï¸ å¤§å­—ãƒ»ä¸ç›®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ãŒã€Shapefileèª­ã¿è¾¼ã¿ã¯æˆåŠŸ")
                # åŸºæœ¬çš„ãªã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                return self._create_basic_area_data_from_gdf(gdf)
                
        except Exception as e:
            st.warning(f"âš ï¸ Shapefileå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

    def _try_process_csv_files(self, extracted_files: List[Dict], temp_dir: str) -> bool:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã‚’è©¦è¡Œ"""
        try:
            csv_files = [f for f in extracted_files if f['extension'] == '.csv']
            
            if not csv_files:
                return False
            
            st.write(f"ğŸ“„ CSVãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {len(csv_files)}å€‹")
            
            # æœ€åˆã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            csv_file = csv_files[0]
            
            # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
            encodings = ['utf-8', 'shift-jis', 'cp932', 'utf-8-sig']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(csv_file['path'], encoding=encoding)
                    st.success(f"âœ… CSVèª­ã¿è¾¼ã¿æˆåŠŸ (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding})")
                    break
                except:
                    continue
            
            if df is None:
                st.error("âŒ CSVèª­ã¿è¾¼ã¿å¤±æ•—ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œï¼‰")
                return False
            
            st.write(f"ğŸ“Š CSVæƒ…å ±:")
            st.write(f"  - è¡Œæ•°: {len(df):,}")
            st.write(f"  - åˆ—æ•°: {len(df.columns)}")
            st.write(f"  - åˆ—å: {', '.join(df.columns[:5])}")
            
            # ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            area_data = self._extract_area_data_from_df(df)
            if area_data:
                st.session_state.area_data = area_data
                st.success(f"âœ… ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º: {len(area_data)}å€‹")
                return True
            
            return False
            
        except Exception as e:
            st.warning(f"âš ï¸ CSVå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

    def _try_process_excel_files(self, extracted_files: List[Dict], temp_dir: str) -> bool:
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã‚’è©¦è¡Œ"""
        try:
            excel_files = [f for f in extracted_files if f['extension'] in ['.xlsx', '.xls']]
            
            if not excel_files:
                return False
            
            st.write(f"ğŸ“Š Excelãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {len(excel_files)}å€‹")
            
            # æœ€åˆã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            excel_file = excel_files[0]
            
            df = pd.read_excel(excel_file['path'])
            
            st.write(f"ğŸ“Š Excelæƒ…å ±:")
            st.write(f"  - è¡Œæ•°: {len(df):,}")
            st.write(f"  - åˆ—æ•°: {len(df.columns)}")
            st.write(f"  - åˆ—å: {', '.join(df.columns[:5])}")
            
            # ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            area_data = self._extract_area_data_from_df(df)
            if area_data:
                st.session_state.area_data = area_data
                st.success(f"âœ… ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º: {len(area_data)}å€‹")
                return True
            
            return False
            
        except Exception as e:
            st.warning(f"âš ï¸ Excelå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

    def _try_process_other_gis_files(self, extracted_files: List[Dict], temp_dir: str) -> bool:
        """ãã®ä»–ã®GISãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã‚’è©¦è¡Œ"""
        try:
            gis_files = [f for f in extracted_files if f['extension'] in ['.kml', '.geojson', '.gpx']]
            
            if not gis_files:
                return False
            
            st.write(f"ğŸ—ºï¸ GISãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {len(gis_files)}å€‹")
            
            # æœ€åˆã®GISãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            gis_file = gis_files[0]
            
            gdf = gpd.read_file(gis_file['path'])
            
            st.write(f"ğŸ“Š GISãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±:")
            st.write(f"  - ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(gdf):,}")
            st.write(f"  - åˆ—æ•°: {len(gdf.columns)}")
            st.write(f"  - åº§æ¨™ç³»: {gdf.crs}")
            
            # ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            area_data = self._extract_area_data_from_gdf(gdf)
            if area_data:
                st.session_state.area_data = area_data
                st.success(f"âœ… ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º: {len(area_data)}å€‹")
                return True
            
            return False
            
        except Exception as e:
            st.warning(f"âš ï¸ GISãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

    def _extract_area_data_from_gdf(self, gdf: gpd.GeoDataFrame) -> Optional[Dict[str, List[str]]]:
        """GeoPandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å¤§å­—ãƒ»ä¸ç›®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        try:
            area_data = {}
            
            st.write("ğŸ” åˆ—ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æ:")
            
            # å…¨åˆ—ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
            for col in gdf.columns:
                if col != 'geometry':
                    unique_values = gdf[col].dropna().unique()
                    st.write(f"  ğŸ“‹ {col}: {len(unique_values)}ç¨®é¡ - {list(unique_values[:5])}{'...' if len(unique_values) > 5 else ''}")
            
            # å¤§å­—ååˆ—ã‚’æ¢ã™ï¼ˆå„ªå…ˆé †ä½ä»˜ãï¼‰
            oaza_columns = []
            priority_keywords = [
                ('å¤§å­—', 10),
                ('oaza', 9),
                ('ç”ºå', 8),
                ('åœ°åŒºå', 7),
                ('åŒºåŸŸ', 6),
                ('name', 5),
                ('åç§°', 5),
                ('åœ°åŸŸ', 4),
                ('area', 3)
            ]
            
            for col in gdf.columns:
                if col == 'geometry':
                    continue
                    
                col_lower = str(col).lower()
                max_priority = 0
                
                for keyword, priority in priority_keywords:
                    if keyword in col_lower:
                        max_priority = max(max_priority, priority)
                
                if max_priority > 0:
                    oaza_columns.append((col, max_priority))
            
            # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
            oaza_columns.sort(key=lambda x: x[1], reverse=True)
            
            if not oaza_columns:
                st.info("â„¹ï¸ å¤§å­—åã«è©²å½“ã™ã‚‹åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            oaza_col = oaza_columns[0][0]
            st.write(f"ğŸï¸ å¤§å­—ååˆ—ã¨ã—ã¦ä½¿ç”¨: {oaza_col} (å„ªå…ˆåº¦: {oaza_columns[0][1]})")
            
            # ä¸ç›®ååˆ—ã‚’æ¢ã™ï¼ˆå„ªå…ˆé †ä½ä»˜ãï¼‰
            chome_columns = []
            chome_keywords = [
                ('ä¸ç›®', 10),
                ('chome', 9),
                ('ç•ªåœ°', 8),
                ('ç•ª', 7),
                ('å°å­—', 6),
                ('koaza', 5),
                ('block', 4)
            ]
            
            for col in gdf.columns:
                if col == 'geometry' or col == oaza_col:
                    continue
                    
                col_lower = str(col).lower()
                max_priority = 0
                
                for keyword, priority in chome_keywords:
                    if keyword in col_lower:
                        max_priority = max(max_priority, priority)
                
                if max_priority > 0:
                    chome_columns.append((col, max_priority))
            
            # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
            chome_columns.sort(key=lambda x: x[1], reverse=True)
            
            # å¤§å­—åã”ã¨ã«ä¸ç›®ã‚’é›†è¨ˆ
            oaza_values = gdf[oaza_col].dropna().unique()
            st.write(f"ğŸ“Š ç™ºè¦‹ã•ã‚ŒãŸå¤§å­—æ•°: {len(oaza_values)}")
            
            for oaza in oaza_values:
                normalized_oaza = self._normalize_area_name(oaza)
                
                if normalized_oaza:
                    area_data[normalized_oaza] = []
                    
                    if chome_columns:
                        chome_col = chome_columns[0][0]
                        st.write(f"ğŸ˜ï¸ ä¸ç›®ååˆ—ã¨ã—ã¦ä½¿ç”¨: {chome_col} (å„ªå…ˆåº¦: {chome_columns[0][1]})")
                        
                        # è©²å½“ã™ã‚‹å¤§å­—ã®ä¸ç›®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                        oaza_data = gdf[gdf[oaza_col] == oaza]
                        chome_values = oaza_data[chome_col].dropna().unique()
                        
                        chome_str_list = []
                        for chome in chome_values:
                            normalized_chome = self._normalize_area_name(chome)
                            if normalized_chome:
                                chome_str_list.append(normalized_chome)
                        
                        area_data[normalized_oaza] = sorted(list(set(chome_str_list))) if chome_str_list else ["ä¸ç›®ãƒ‡ãƒ¼ã‚¿ãªã—"]
                    else:
                        area_data[normalized_oaza] = ["ä¸ç›®ãƒ‡ãƒ¼ã‚¿ãªã—"]
            
            # ç©ºã®ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–
            area_data = {k: v for k, v in area_data.items() if k and v}
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šæŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
            st.write("ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸå¤§å­—ãƒ»ä¸ç›®ãƒ‡ãƒ¼ã‚¿:")
            for oaza, chome_list in list(area_data.items())[:5]:
                st.write(f"  ğŸï¸ {oaza}: {', '.join(chome_list[:3])}{'...' if len(chome_list) > 3 else ''}")
            if len(area_data) > 5:
                st.write(f"  ... ä»–{len(area_data)-5}å€‹")
            
            return area_data if area_data else None
            
        except Exception as e:
            st.error(f"âŒ GDFå¤§å­—ãƒ»ä¸ç›®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None

    def _extract_area_data_from_df(self, df: pd.DataFrame) -> Optional[Dict[str, List[str]]]:
        """Pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰å¤§å­—ãƒ»ä¸ç›®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        try:
            area_data = {}
            
            st.write("ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åˆ—ã®è©³ç´°åˆ†æ:")
            
            # å…¨åˆ—ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
            for col in df.columns:
                unique_values = df[col].dropna().unique()
                st.write(f"  ğŸ“‹ {col}: {len(unique_values)}ç¨®é¡ - {list(unique_values[:5])}{'...' if len(unique_values) > 5 else ''}")
            
            # å¤§å­—ååˆ—ã‚’æ¢ã™ï¼ˆå„ªå…ˆé †ä½ä»˜ãï¼‰
            oaza_columns = []
            priority_keywords = [
                ('å¤§å­—', 10),
                ('oaza', 9),
                ('ç”ºå', 8),
                ('åœ°åŒºå', 7),
                ('åŒºåŸŸ', 6),
                ('name', 5),
                ('åç§°', 5),
                ('åœ°åŸŸ', 4),
                ('area', 3)
            ]
            
            for col in df.columns:
                col_lower = str(col).lower()
                max_priority = 0
                
                for keyword, priority in priority_keywords:
                    if keyword in col_lower:
                        max_priority = max(max_priority, priority)
                
                if max_priority > 0:
                    oaza_columns.append((col, max_priority))
            
            # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
            oaza_columns.sort(key=lambda x: x[1], reverse=True)
            
            if not oaza_columns:
                st.info("â„¹ï¸ å¤§å­—åã«è©²å½“ã™ã‚‹åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            oaza_col = oaza_columns[0][0]
            st.write(f"ğŸï¸ å¤§å­—ååˆ—ã¨ã—ã¦ä½¿ç”¨: {oaza_col} (å„ªå…ˆåº¦: {oaza_columns[0][1]})")
            
            # ä¸ç›®ååˆ—ã‚’æ¢ã™
            chome_columns = []
            chome_keywords = [
                ('ä¸ç›®', 10),
                ('chome', 9),
                ('ç•ªåœ°', 8),
                ('ç•ª', 7),
                ('å°å­—', 6),
                ('koaza', 5),
                ('block', 4)
            ]
            
            for col in df.columns:
                if col == oaza_col:
                    continue
                    
                col_lower = str(col).lower()
                max_priority = 0
                
                for keyword, priority in chome_keywords:
                    if keyword in col_lower:
                        max_priority = max(max_priority, priority)
                
                if max_priority > 0:
                    chome_columns.append((col, max_priority))
            
            # å¤§å­—åä¸€è¦§ã‚’å–å¾—ã—æ­£è¦åŒ–
            oaza_values = df[oaza_col].dropna().unique()
            st.write(f"ğŸ“Š ç™ºè¦‹ã•ã‚ŒãŸå¤§å­—å€™è£œæ•°: {len(oaza_values)}")
            
            for oaza in oaza_values:
                normalized_oaza = self._normalize_area_name(oaza)
                
                if normalized_oaza:
                    if chome_columns:
                        chome_col = chome_columns[0][0]
                        st.write(f"ğŸ˜ï¸ ä¸ç›®ååˆ—ã¨ã—ã¦ä½¿ç”¨: {chome_col}")
                        
                        # è©²å½“ã™ã‚‹å¤§å­—ã®ä¸ç›®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                        oaza_data = df[df[oaza_col] == oaza]
                        chome_values = oaza_data[chome_col].dropna().unique()
                        
                        chome_str_list = []
                        for chome in chome_values:
                            normalized_chome = self._normalize_area_name(chome)
                            if normalized_chome:
                                chome_str_list.append(normalized_chome)
                        
                        area_data[normalized_oaza] = sorted(list(set(chome_str_list))) if chome_str_list else ["ä¸ç›®ãƒ‡ãƒ¼ã‚¿ãªã—"]
                    else:
                        area_data[normalized_oaza] = ["ä¸ç›®ãƒ‡ãƒ¼ã‚¿ãªã—"]
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šæŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
            st.write("ğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸå¤§å­—ãƒ»ä¸ç›®ãƒ‡ãƒ¼ã‚¿:")
            for oaza, chome_list in list(area_data.items())[:5]:
                st.write(f"  ğŸï¸ {oaza}: {', '.join(chome_list[:3])}{'...' if len(chome_list) > 3 else ''}")
            if len(area_data) > 5:
                st.write(f"  ... ä»–{len(area_data)-5}å€‹")
            
            return area_data if area_data else None
            
        except Exception as e:
            st.error(f"âŒ DFå¤§å­—ãƒ»ä¸ç›®ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None

    def _create_basic_area_data_from_gdf(self, gdf: gpd.GeoDataFrame) -> bool:
        """GeoDataFrameã‹ã‚‰åŸºæœ¬çš„ãªã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        try:
            # åœ°åŸŸã«é–¢é€£ã™ã‚‹åˆ—ã‚’æ¢ã™
            area_columns = []
            keywords = [
                ('name', 10),
                ('å', 9),
                ('åœ°åŒº', 8),
                ('åŒºåŸŸ', 7),
                ('ç”º', 6),
                ('æ‘', 5),
                ('area', 4)
            ]
            
            for col in gdf.columns:
                if col == 'geometry':
                    continue
                    
                col_lower = str(col).lower()
                max_priority = 0
                
                for keyword, priority in keywords:
                    if keyword in col_lower:
                        max_priority = max(max_priority, priority)
                
                if max_priority > 0:
                    area_columns.append((col, max_priority))
            
            # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
            area_columns.sort(key=lambda x: x[1], reverse=True)
            
            if area_columns:
                area_col = area_columns[0][0]
                unique_areas = gdf[area_col].dropna().unique()
                
                area_data = {}
                for area in unique_areas[:20]:  # æœ€å¤§20å€‹ã¾ã§
                    normalized_area = self._normalize_area_name(area)
                    if normalized_area:
                        area_data[normalized_area] = ["ãƒ‡ãƒ¼ã‚¿ãªã—"]
                
                if area_data:
                    st.session_state.area_data = area_data
                    st.success(f"âœ… åŸºæœ¬ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ: {len(area_data)}å€‹")
                    return True
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
            return self._create_dummy_area_data("Shapefile")
            
        except Exception as e:
            st.warning(f"âš ï¸ åŸºæœ¬ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return self._create_dummy_area_data("Shapefile")

    def _create_dummy_area_data(self, source_name: str) -> bool:
        """ãƒ€ãƒŸãƒ¼ã®å¤§å­—ãƒ»ä¸ç›®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        try:
            # æ²–ç¸„çœŒã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿåœ¨ã®å¤§å­—ãƒ»ä¸ç›®ï¼‰
            dummy_area_data = {
                "é‚£è¦‡": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®"],
                "é¦–é‡Œ": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®", "4ä¸ç›®", "5ä¸ç›®"],
                "çœŸå˜‰æ¯”": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®"],
                "æ³Š": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®"],
                "ä¹…èŒ‚åœ°": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®"],
                "ç‰§å¿—": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®"],
                "å®‰é‡Œ": ["1ä¸ç›®", "2ä¸ç›®"],
                "ä¸ŠåŸ": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®"],
                "å®®é‡Œ": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®", "4ä¸ç›®"],
                "æ™®å¤©é–“": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®", "4ä¸ç›®"],
                "å†…é–“": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®"],
                "çµŒå¡š": ["1ä¸ç›®", "2ä¸ç›®"],
                "å¤§å±±": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®", "4ä¸ç›®", "5ä¸ç›®", "6ä¸ç›®", "7ä¸ç›®"],
                "å®œé‡æ¹¾": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®"],
                "æ–°åŸ": ["1ä¸ç›®", "2ä¸ç›®"],
                "æˆ‘å¦‚å¤": ["1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®", "4ä¸ç›®"]
            }
            
            st.session_state.area_data = dummy_area_data
            st.warning(f"âš ï¸ {source_name}ã‹ã‚‰é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            st.info(f"ğŸ’¡ ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ç¶™ç¶šã—ã¾ã™ï¼ˆ{len(dummy_area_data)}å€‹ã®å¤§å­—ï¼‰")
            
            return True
            
        except Exception as e:
            st.error(f"âŒ ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

    def _process_data_file(self, file_content: bytes, file_extension: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆCSV/Excelï¼‰ã®å‡¦ç†"""
        try:
            if file_extension == '.csv':
                # è¤‡æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œ
                encodings = ['utf-8', 'shift-jis', 'cp932', 'utf-8-sig']
                df = None
                
                for encoding in encodings:
                    try:
                        df = pd.read_csv(io.BytesIO(file_content), encoding=encoding)
                        st.success(f"âœ… CSVèª­ã¿è¾¼ã¿æˆåŠŸ (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding})")
                        break
                    except:
                        continue
                
                if df is None:
                    st.error("âŒ CSVèª­ã¿è¾¼ã¿å¤±æ•—")
                    return False
                    
            else:  # Excel
                df = pd.read_excel(io.BytesIO(file_content))
            
            st.write(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±:")
            st.write(f"  - è¡Œæ•°: {len(df):,}")
            st.write(f"  - åˆ—æ•°: {len(df.columns)}")
            
            # ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            area_data = self._extract_area_data_from_df(df)
            if area_data:
                st.session_state.area_data = area_data
                st.success(f"âœ… ã‚¨ãƒªã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º: {len(area_data)}å€‹")
                return True
            
            return self._create_dummy_area_data("ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«")
            
        except Exception as e:
            st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False

    def _process_shapefile(self, file_content: bytes, file_name: str) -> bool:
        """å˜ä½“Shapefileã®å‡¦ç†"""
        try:
            with tempfile.NamedTemporaryFile(suffix='.shp', delete=False) as temp_file:
                temp_file.write(file_content)
                temp_file.flush()
                
                gdf = gpd.read_file(temp_file.name)
                
                area_data = self._extract_area_data_from_gdf(gdf)
                if area_data:
                    st.session_state.area_data = area_data
                    st.success(f"âœ… Shapefileå‡¦ç†å®Œäº†: {len(area_data)}å€‹")
                    return True
                
                return self._create_dummy_area_data("Shapefile")
                
        except Exception as e:
            st.error(f"âŒ Shapefileå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
        finally:
            try:
                os.unlink(temp_file.name)
            except:
                pass

    def _process_gis_file(self, file_content: bytes, file_extension: str) -> bool:
        """GISãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆKML/GeoJSONï¼‰ã®å‡¦ç†"""
        try:
            with tempfile.NamedTemporaryFile(suffix=file_extension, delete=False) as temp_file:
                temp_file.write(file_content)
                temp_file.flush()
                
                gdf = gpd.read_file(temp_file.name)
                
                area_data = self._extract_area_data_from_gdf(gdf)
                if area_data:
                    st.session_state.area_data = area_data
                    st.success(f"âœ… GISãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {len(area_data)}å€‹")
                    return True
                
                return self._create_dummy_area_data("GISãƒ•ã‚¡ã‚¤ãƒ«")
                
        except Exception as e:
            st.error(f"âŒ GISãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
        finally:
            try:
                os.unlink(temp_file.name)
            except:
                pass